def evaluate_content_by_type(content: str, content_type: str, custom_requirements: str = \"\") -> Dict:\n    \"\"\"\n    Evaluate content based on its type with appropriate metrics.\n    \n    Args:\n        content: The content to evaluate\n        content_type: Type of content ('code_python', 'code_js', 'code_java', 'protocol', 'documentation', 'other')\n        custom_requirements: Additional requirements to check for\n        \n    Returns:\n        Dict: Evaluation results with scores and recommendations\n    \"\"\"\n    if content_type.startswith('code_'):\n        return _evaluate_code_content(content, content_type, custom_requirements)\n    elif content_type in ['protocol', 'documentation']:\n        return _evaluate_document_content(content, content_type, custom_requirements)\n    else:\n        return _evaluate_general_content(content, custom_requirements)\n\n\ndef _evaluate_code_content(content: str, content_type: str, custom_requirements: str = \"\") -> Dict:\n    \"\"\"Evaluate code content with language-specific metrics.\"\"\"\n    import re\n    \n    evaluation = {\n        \"valid\": True,\n        \"score\": 0,\n        \"errors\": [],\n        \"warnings\": [],\n        \"suggestions\": [],\n        \"complexity_metrics\": calculate_protocol_complexity(content),\n        \"language_specific\": {}\n    }\n    \n    # Check for basic structural issues\n    if not content.strip():\n        evaluation[\"valid\"] = False\n        evaluation[\"errors\"].append(\"Content is empty\")\n        evaluation[\"score\"] = 0\n        return evaluation\n    \n    # Language-specific checks with enhanced patterns\n    language_patterns = {\n        'code_python': {\n            'imports': r'import\\s+\\w+|from\\s+\\w+\\s+import',\n            'functions': r'def\\s+\\w+\\s*\\(',\n            'classes': r'class\\s+\\w+\\s*[:\\(]',\n            'comments': r'#.*\n\n\ndef _evaluate_document_content(content: str, content_type: str, custom_requirements: str = \"\") -> Dict:\n    \"\"\"Evaluate document content (protocols, documentation).\"\"\"\n    evaluation = {\n        \"valid\": True,\n        \"score\": 0,\n        \"errors\": [],\n        \"warnings\": [],\n        \"suggestions\": [],\n        \"complexity_metrics\": calculate_protocol_complexity(content),\n        \"structure_analysis\": extract_protocol_structure(content)\n    }\n    \n    if not content.strip():\n        evaluation[\"valid\"] = False\n        evaluation[\"errors\"].append(\"Content is empty\")\n        evaluation[\"score\"] = 0\n        return evaluation\n    \n    # Structure checks for documents\n    structure = evaluation[\"structure_analysis\"]\n    \n    if not structure[\"has_headers\"]:\n        evaluation[\"suggestions\"].append(\"Add headers to organize the content\")\n    \n    if not structure[\"has_numbered_steps\"] and not structure[\"has_bullet_points\"]:\n        evaluation[\"suggestions\"].append(\"Consider using numbered steps or bullet points for better readability\")\n    \n    if not structure[\"has_preconditions\"] and content_type == \"protocol\":\n        evaluation[\"suggestions\"].append(\"Consider adding preconditions that must be met before following this protocol\")\n    \n    if not structure[\"has_postconditions\"] and content_type == \"protocol\":\n        evaluation[\"suggestions\"].append(\"Consider adding postconditions to define expected outcomes\")\n    \n    if not structure[\"has_error_handling\"]:\n        evaluation[\"suggestions\"].append(\"Consider adding error handling or contingency procedures\")\n    \n    # Calculate score based on structure and content\n    structure_score = (\n        (1 if structure[\"has_headers\"] else 0) * 20 +\n        (1 if structure[\"has_numbered_steps\"] or structure[\"has_bullet_points\"] else 0) * 20 +\n        (1 if structure[\"has_preconditions\"] else 0) * 15 +\n        (1 if structure[\"has_postconditions\"] else 0) * 15 +\n        (1 if structure[\"has_error_handling\"] else 0) * 15 +\n        min(15, structure[\"section_count\"] * 3)  # Up to 15 points for sections\n    )\n    \n    # Adjust for readability\n    complexity = evaluation[\"complexity_metrics\"]\n    if complexity[\"avg_sentence_length\"] > 25:\n        evaluation[\"suggestions\"].append(\"Consider shortening sentences for better readability\")\n    \n    if complexity[\"unique_words\"] / max(1, complexity[\"word_count\"]) < 0.4:\n        evaluation[\"suggestions\"].append(\"Increase vocabulary diversity to improve clarity\")\n    \n    evaluation[\"score\"] = min(100, structure_score)\n    \n    return evaluation\n\n\ndef _evaluate_general_content(content: str, custom_requirements: str = \"\") -> Dict:\n    \"\"\"Evaluate general content without specific type requirements.\"\"\"\n    evaluation = {\n        \"valid\": True,\n        \"score\": 0,\n        \"errors\": [],\n        \"warnings\": [],\n        \"suggestions\": [],\n        \"complexity_metrics\": calculate_protocol_complexity(content)\n    }\n    \n    if not content.strip():\n        evaluation[\"valid\"] = False\n        evaluation[\"errors\"].append(\"Content is empty\")\n        evaluation[\"score\"] = 0\n        return evaluation\n    \n    # General quality checks\n    complexity = evaluation[\"complexity_metrics\"]\n    \n    # Calculate score based on readability and structure\n    readability_score = 100 - (complexity[\"avg_sentence_length\"] / 50 * 100)\n    readability_score = max(0, min(100, readability_score))\n    \n    length_score = min(30, complexity[\"word_count\"] / 100 * 10)  # Up to 30 points for length\n    \n    # Vocabulary diversity score\n    vocab_diversity = complexity[\"unique_words\"] / max(1, complexity[\"word_count\"])\n    vocab_score = min(20, vocab_diversity * 100)\n    \n    evaluation[\"score\"] = min(100, int(readability_score * 0.3 + length_score * 0.4 + vocab_score * 0.3))\n    \n    # Add suggestions based on analysis\n    if complexity[\"avg_sentence_length\"] > 25:\n        evaluation[\"suggestions\"].append(\"Consider shortening sentences for better readability\")\n    \n    if complexity[\"word_count\"] < 100:\n        evaluation[\"suggestions\"].append(\"Content might be too brief - consider adding more detail\")\n    \n    if vocab_diversity < 0.4:\n        evaluation[\"suggestions\"].append(\"Consider using a more diverse vocabulary\")\n    \n    return evaluation\n\n\ndef enhance_user_experience():\n    \"\"\"\n    Enhance the user experience by providing better feedback and options\n    based on content type and evaluation results.\n    \"\"\"\n    # This function can be called to improve the UI/UX of the application\n    # based on content analysis and user preferences\n    pass,
            'specific_checks': [\n                (lambda c: 'import' not in c.lower(), \"Consider organizing imports at the top of the file\"),\n                (lambda c: re.search(r'if\\s+__name__\\s*==\\s*[\"']__main__[\"']', c), \"Good use of main guard pattern\")\n            ]\n        },\n        'code_js': {\n            'imports': r'import\\s+.*|require\\(.*\\)',\n            'functions': r'function\\s+\\w+|const\\s+\\w+\\s*=.*=>|let\\s+\\w+\\s*=.*=>',\n            'classes': r'class\\s+\\w+\\s*{',\n            'comments': r'//.*$|/\\*.*?\\*/',\n            'specific_checks': [\n                (lambda c: not any(kw in c for kw in ['function', '=>', 'class', 'const', 'let', 'var']), \n                 \"This doesn't appear to contain JavaScript code\"),\n                (lambda c: 'use strict' in c, \"Good use of strict mode\")\n            ]\n        },\n        'code_java': {\n            'imports': r'import\\s+.*;',\n            'functions': r'(public|private|protected)\\s+(static\\s+)?\\w+\\s+\\w+\\s*\\(',\n            'classes': r'(public\\s+)?class\\s+\\w+',\n            'comments': r'//.*$|/\\*.*?\\*/',\n            'specific_checks': [\n                (lambda c: not any(kw in c for kw in ['public class', 'class']), \n                 \"Standard Java files should contain a public class\"),\n                (lambda c: 'static void main' in c, \"Contains main method entry point\")\n            ]\n        },\n        'code_cpp': {\n            'imports': r'#include\\s*<\\w+>',\n            'functions': r'\\w+\\s+\\w+\\s*\\([^)]*\\)\\s*{',\n            'classes': r'(class|struct)\\s+\\w+',\n            'comments': r'//.*$|/\\*.*?\\*/',\n            'specific_checks': [\n                (lambda c: '#include' not in c, \"Consider adding include directives for required libraries\"),\n                (lambda c: 'std::' in c, \"Using standard library components\")\n            ]\n        },\n        'code_csharp': {\n            'imports': r'using\\s+\\w+(\\.\\w+)*;',\n            'functions': r'(public|private|protected)\\s+(static\\s+)?\\w+\\s+\\w+\\s*\\(',\n            'classes': r'class\\s+\\w+',\n            'comments': r'//.*$|/\\*.*?\\*/',\n            'specific_checks': [\n                (lambda c: 'namespace' not in c, \"Consider organizing code in a namespace\"),\n                (lambda c: 'Console\\.' in c, \"Using console for output\")\n            ]\n        },\n        'code_go': {\n            'imports': r'import\\s+',\n            'functions': r'func\\s+\\w+\\s*\\(',\n            'classes': r'type\\s+\\w+\\s+(struct|interface)',\n            'comments': r'//.*$|/\\*.*?\\*/',\n            'specific_checks': [\n                (lambda c: 'package ' not in c, \"Consider adding package declaration\"),\n                (lambda c: 'fmt\\.' in c, \"Using fmt package for output\")\n            ]\n        },\n        'code_rust': {\n            'imports': r'use\\s+\\w+',\n            'functions': r'fn\\s+\\w+\\s*\\(',\n            'classes': r'struct\\s+\\w+|enum\\s+\\w+',\n            'comments': r'//.*$|/\\*.*?\\*/',\n            'specific_checks': [\n                (lambda c: 'fn main' not in c, \"Consider adding main function for executable programs\"),\n                (lambda c: 'println!' in c, \"Using println! macro for output\")\n            ]\n        }\n    }\n    \n    # Get patterns for the specific language or default to Python\n    patterns = language_patterns.get(content_type, language_patterns['code_python'])\n    \n    # Count language-specific elements\n    imports = len(re.findall(patterns['imports'], content, re.MULTILINE))\n    functions = len(re.findall(patterns['functions'], content))\n    classes = len(re.findall(patterns['classes'], content))\n    comments = len(re.findall(patterns['comments'], content, re.MULTILINE | re.DOTALL))\n    \n    evaluation[\"language_specific\"] = {\n        \"imports\": imports,\n        \"functions\": functions,\n        \"classes\": classes,\n        \"comments\": comments\n    }\n    \n    # Calculate quality metrics\n    lines = content.split('\\n')\n    total_lines = len(lines)\n    empty_lines = len([line for line in lines if not line.strip()])\n    code_lines = total_lines - empty_lines\n    \n    # Check for common issues\n    if functions == 0 and classes == 0:\n        evaluation[\"warnings\"].append(\"No functions or classes found - this file might be empty or only contain imports\")\n    \n    if comments == 0:\n        evaluation[\"suggestions\"].append(\"Consider adding comments to explain complex code\")\n    \n    # Basic correctness check based on structure\n    balance_score = 0\n    open_parens = content.count('(')\n    close_parens = content.count(')')\n    open_braces = content.count('{')\n    close_braces = content.count('}')\n    open_brackets = content.count('[')\n    close_brackets = content.count(']')\n    \n    if open_parens == close_parens and open_braces == close_braces and open_brackets == close_brackets:\n        balance_score = 100\n    else:\n        evaluation[\"errors\"].append(\"Unbalanced parentheses, braces, or brackets\")\n    \n    # Language-specific checks\n    for check_func, suggestion in patterns.get('specific_checks', []):\n        try:\n            if check_func(content):\n                if \"doesn't appear\" in suggestion or \"should\" in suggestion:\n                    evaluation[\"warnings\"].append(suggestion)\n                else:\n                    evaluation[\"suggestions\"].append(suggestion)\n        except:\n            pass  # Skip invalid checks\n    \n    # Calculate overall score (0-100)\n    structure_score = min(100, (functions * 20 + classes * 15 + comments * 5) / max(1, total_lines) * 10)\n    complexity_score = evaluation[\"complexity_metrics\"][\"complexity_score\"]\n    \n    # Adjust score based on language specifics\n    # Combine metrics\n    evaluation[\"score\"] = min(100, int((structure_score * 0.4 + balance_score * 0.3 + (100 - complexity_score) * 0.3)))\n    \n    return evaluation\n\n\ndef _evaluate_document_content(content: str, content_type: str, custom_requirements: str = \"\") -> Dict:\n    \"\"\"Evaluate document content (protocols, documentation).\"\"\"\n    evaluation = {\n        \"valid\": True,\n        \"score\": 0,\n        \"errors\": [],\n        \"warnings\": [],\n        \"suggestions\": [],\n        \"complexity_metrics\": calculate_protocol_complexity(content),\n        \"structure_analysis\": extract_protocol_structure(content)\n    }\n    \n    if not content.strip():\n        evaluation[\"valid\"] = False\n        evaluation[\"errors\"].append(\"Content is empty\")\n        evaluation[\"score\"] = 0\n        return evaluation\n    \n    # Structure checks for documents\n    structure = evaluation[\"structure_analysis\"]\n    \n    if not structure[\"has_headers\"]:\n        evaluation[\"suggestions\"].append(\"Add headers to organize the content\")\n    \n    if not structure[\"has_numbered_steps\"] and not structure[\"has_bullet_points\"]:\n        evaluation[\"suggestions\"].append(\"Consider using numbered steps or bullet points for better readability\")\n    \n    if not structure[\"has_preconditions\"] and content_type == \"protocol\":\n        evaluation[\"suggestions\"].append(\"Consider adding preconditions that must be met before following this protocol\")\n    \n    if not structure[\"has_postconditions\"] and content_type == \"protocol\":\n        evaluation[\"suggestions\"].append(\"Consider adding postconditions to define expected outcomes\")\n    \n    if not structure[\"has_error_handling\"]:\n        evaluation[\"suggestions\"].append(\"Consider adding error handling or contingency procedures\")\n    \n    # Calculate score based on structure and content\n    structure_score = (\n        (1 if structure[\"has_headers\"] else 0) * 20 +\n        (1 if structure[\"has_numbered_steps\"] or structure[\"has_bullet_points\"] else 0) * 20 +\n        (1 if structure[\"has_preconditions\"] else 0) * 15 +\n        (1 if structure[\"has_postconditions\"] else 0) * 15 +\n        (1 if structure[\"has_error_handling\"] else 0) * 15 +\n        min(15, structure[\"section_count\"] * 3)  # Up to 15 points for sections\n    )\n    \n    # Adjust for readability\n    complexity = evaluation[\"complexity_metrics\"]\n    if complexity[\"avg_sentence_length\"] > 25:\n        evaluation[\"suggestions\"].append(\"Consider shortening sentences for better readability\")\n    \n    if complexity[\"unique_words\"] / max(1, complexity[\"word_count\"]) < 0.4:\n        evaluation[\"suggestions\"].append(\"Increase vocabulary diversity to improve clarity\")\n    \n    evaluation[\"score\"] = min(100, structure_score)\n    \n    return evaluation\n\n\ndef _evaluate_general_content(content: str, custom_requirements: str = \"\") -> Dict:\n    \"\"\"Evaluate general content without specific type requirements.\"\"\"\n    evaluation = {\n        \"valid\": True,\n        \"score\": 0,\n        \"errors\": [],\n        \"warnings\": [],\n        \"suggestions\": [],\n        \"complexity_metrics\": calculate_protocol_complexity(content)\n    }\n    \n    if not content.strip():\n        evaluation[\"valid\"] = False\n        evaluation[\"errors\"].append(\"Content is empty\")\n        evaluation[\"score\"] = 0\n        return evaluation\n    \n    # General quality checks\n    complexity = evaluation[\"complexity_metrics\"]\n    \n    # Calculate score based on readability and structure\n    readability_score = 100 - (complexity[\"avg_sentence_length\"] / 50 * 100)\n    readability_score = max(0, min(100, readability_score))\n    \n    length_score = min(30, complexity[\"word_count\"] / 100 * 10)  # Up to 30 points for length\n    \n    # Vocabulary diversity score\n    vocab_diversity = complexity[\"unique_words\"] / max(1, complexity[\"word_count\"])\n    vocab_score = min(20, vocab_diversity * 100)\n    \n    evaluation[\"score\"] = min(100, int(readability_score * 0.3 + length_score * 0.4 + vocab_score * 0.3))\n    \n    # Add suggestions based on analysis\n    if complexity[\"avg_sentence_length\"] > 25:\n        evaluation[\"suggestions\"].append(\"Consider shortening sentences for better readability\")\n    \n    if complexity[\"word_count\"] < 100:\n        evaluation[\"suggestions\"].append(\"Content might be too brief - consider adding more detail\")\n    \n    if vocab_diversity < 0.4:\n        evaluation[\"suggestions\"].append(\"Consider using a more diverse vocabulary\")\n    \n    return evaluation\n\n\ndef enhance_user_experience():\n    \"\"\"\n    Enhance the user experience by providing better feedback and options\n    based on content type and evaluation results.\n    \"\"\"\n    # This function can be called to improve the UI/UX of the application\n    # based on content analysis and user preferences\n    pass